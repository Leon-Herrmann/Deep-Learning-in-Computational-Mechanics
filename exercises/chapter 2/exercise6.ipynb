{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6 - Adam\n",
    "### Task\n",
    "Implement the Adam optimizer. For help refer to algorithm 2. A class structure is provided. After the implementation, compare the Adam optimizer with standard gradient descent \n",
    "\n",
    "### Learning goals\n",
    "- Understand the Adam optimizer\n",
    "- Experience the difference between Adam and standard gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "import copy"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define starting position and function to optimize"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "np.random.seed(100)  # Generate the data.\n",
    "x1 = 4.0\n",
    "x2 = 4.0\n",
    "params0 = [np.array([x1]), np.array([x2])]  # initial guess\n",
    "\n",
    "f = lambda x1, x2: 100 * (x2 - x1**2) ** 2 + (1 - x1) ** 2  # Rosenbrock function\n",
    "dfdx = lambda x1, x2: [\n",
    "    np.array(400 * (-x2 + x1**2) * x1 + 2 * (x1 - 1)),\n",
    "    np.array(200 * (x2 - x1**2)),\n",
    "]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "adam optimizer "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class AdamOptimizer:\n",
    "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.m = None  # list to store all first statistical moments\n",
    "        self.n = None  # list to store all second statistical moments\n",
    "        self.t = 0  # keeps track of how many epochs have been performed\n",
    "\n",
    "    def updateParams(self, params, grads):\n",
    "        if self.m is None:\n",
    "            self.m = [np.zeros_like(param) for param in params]  # initializing list\n",
    "        if self.n is None:\n",
    "            self.n = [np.zeros_like(param) for param in params]  # initializing list\n",
    "\n",
    "        updatedParams = []\n",
    "\n",
    "        raise NotImplementedError()  # your code goes here\n",
    "\n",
    "        return updatedParams"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimization with gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "lr = 2e-4\n",
    "epochs = 4000\n",
    "\n",
    "params = copy.deepcopy(params0)\n",
    "\n",
    "optimizationPathGD = np.zeros((2, epochs))\n",
    "for epoch in range(epochs):\n",
    "    cost = f(params[0], params[1]).item()\n",
    "    optimizationPathGD[0, epoch] = params[0]\n",
    "    optimizationPathGD[1, epoch] = params[0]\n",
    "    grad = dfdx(params[0], params[1])\n",
    "    params[0] -= lr * grad[0]\n",
    "    params[1] -= lr * grad[1]\n",
    "    if epoch % 100 == 0:\n",
    "        string = \"Epoch: {}/{}\\t\\tCost = {:.2e}\"\n",
    "        print(string.format(epoch, epochs, cost))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "optimization with Adam"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "lr = 5e-1\n",
    "epochs = 4000\n",
    "optimizer = AdamOptimizer(lr=lr)\n",
    "\n",
    "params = copy.deepcopy(params0)\n",
    "\n",
    "optimizationPathAdam = np.zeros((2, epochs))\n",
    "for epoch in range(epochs):\n",
    "    cost = f(params[0], params[1]).item()\n",
    "    optimizationPathAdam[0, epoch] = params[0]\n",
    "    optimizationPathAdam[1, epoch] = params[0]\n",
    "    grad = dfdx(params[0], params[1])\n",
    "\n",
    "    params = optimizer.updateParams(params, grad)\n",
    "\n",
    "    # params[0] -= lr * grad[0]\n",
    "    # params[1] -= lr * grad[1]\n",
    "    if epoch % 100 == 0:\n",
    "        string = \"Epoch: {}/{}\\t\\tCost = {:.2e}\"\n",
    "        print(string.format(epoch, epochs, cost))"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "visualize the optimization"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "x1_ = np.linspace(-4, 4, 200)\n",
    "x2_ = np.linspace(-3, 5, 200)\n",
    "x1_, x2_ = np.meshgrid(x1_, x2_, indexing=\"ij\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "cp = ax.pcolormesh(x1_, x2_, f(x1_, x2_), cmap=plt.cm.jet, norm=colors.LogNorm())\n",
    "ax.plot(\n",
    "    optimizationPathGD[0],\n",
    "    optimizationPathGD[1],\n",
    "    \"ko\",\n",
    "    markersize=12,\n",
    "    label=\"gradient descent\",\n",
    ")\n",
    "ax.plot(\n",
    "    optimizationPathAdam[0], optimizationPathAdam[1], \"r+\", markersize=12, label=\"Adam\"\n",
    ")\n",
    "ax.plot([1], [1], \"bs\", markersize=12, label=\"minimum\")\n",
    "fig.colorbar(cp)\n",
    "ax.legend()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
